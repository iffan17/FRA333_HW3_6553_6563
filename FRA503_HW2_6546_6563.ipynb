{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iffan17/FRA333_HW3_6553_6563/blob/main/FRA503_HW2_6546_6563.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E2syKODzJy7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FRA502 Deep Reinforcement Learning For Robotics\n",
        "## ‡∏†‡∏ô‡∏•‡∏†‡∏±‡∏™ ‡∏™‡∏∏‡∏ó‡∏ò‡∏¥‡∏°‡∏≤‡∏•‡∏≤ 65340500046\n",
        "## ‡∏≠‡∏¥‡∏ü‡∏ü‡∏≤‡∏ô ‡∏≠‡∏±‡∏Ñ‡∏£‡∏≤‡∏°‡∏µ‡∏ô 65340500063"
      ],
      "metadata": {
        "id": "_sWKDk1vIOju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üìä \"Data Analysis Module\" (Sneaky Number Guesser)\n",
        "import random\n",
        "\n",
        "def fake_data_report():\n",
        "    print(\"üìà Calculating metrics...\")\n",
        "    secret = random.randint(1, 100)\n",
        "    tries = 0\n",
        "\n",
        "    while True:\n",
        "        guess = input(\"Enter dataset ID (1-100): \").strip()\n",
        "        if not guess.isdigit():\n",
        "            print(\"‚ö†Ô∏è Invalid input format.\")\n",
        "            continue\n",
        "        guess = int(guess)\n",
        "        tries += 1\n",
        "\n",
        "        if guess < secret:\n",
        "            print(\"‚û°Ô∏è Higher variance detected!\")\n",
        "        elif guess > secret:\n",
        "            print(\"‚¨ÖÔ∏è Lower trend observed!\")\n",
        "        else:\n",
        "            print(f\"‚úÖ Anomaly found in {tries} attempts!\")\n",
        "            break\n",
        "\n",
        "fake_data_report()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUo4PYa7ddoB",
        "outputId": "f6a2f84f-b764-4fe7-cf4c-ca9418d9f4c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìà Calculating metrics...\n",
            "Enter dataset ID (1-100): 99\n",
            "‚¨ÖÔ∏è Lower trend observed!\n",
            "Enter dataset ID (1-100): 23\n",
            "‚û°Ô∏è Higher variance detected!\n",
            "Enter dataset ID (1-100): 58\n",
            "‚û°Ô∏è Higher variance detected!\n",
            "Enter dataset ID (1-100): 69\n",
            "‚û°Ô∏è Higher variance detected!\n",
            "Enter dataset ID (1-100): 89\n",
            "‚¨ÖÔ∏è Lower trend observed!\n",
            "Enter dataset ID (1-100): 70\n",
            "‚û°Ô∏è Higher variance detected!\n",
            "Enter dataset ID (1-100): 75\n",
            "‚¨ÖÔ∏è Lower trend observed!\n",
            "Enter dataset ID (1-100): 73\n",
            "‚úÖ Anomaly found in 8 attempts!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 1: Setting up `Cart-Pole` Agent**"
      ],
      "metadata": {
        "id": "oPPTba82ceLy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. RL Base Class**"
      ],
      "metadata": {
        "id": "f8O9Ht9Fcj0K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á Base Class ‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á Reinforcement Learning (RL) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Cart-Pole ‡∏ã‡∏∂‡πà‡∏á‡∏à‡∏∞‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î **Constructor** ‡πÅ‡∏•‡∏∞ **Core Functions** ‡∏ï‡∏≤‡∏°‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î"
      ],
      "metadata": {
        "id": "EzEpMp2ZcnDt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Constructor**"
      ],
      "metadata": {
        "id": "Xhy-zdnWcoUv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Constructor ‡∏à‡∏∞‡∏°‡∏µ‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç ‡πÄ‡∏ä‡πà‡∏ô:\n",
        "- **Control type**: ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô RL Algorithm ‡πÅ‡∏ö‡∏ö‡πÉ‡∏î (‡πÄ‡∏ä‡πà‡∏ô Q-Learning, SARSA, Double Q-Learning)\n",
        "- **Number of actions**: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô discrete actions ‡∏ó‡∏µ‡πà agent ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏î‡πâ\n",
        "- **Action range**: ‡∏ä‡πà‡∏ß‡∏á‡∏Ñ‡πà‡∏≤‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î (min) ‡πÅ‡∏•‡∏∞‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (max) ‡∏Ç‡∏≠‡∏á action (continuous)\n",
        "- **Discretize state weight**: ‡πÉ‡∏ä‡πâ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Å‡∏≤‡∏£ discretize state (‡∏Å‡∏≥‡∏´‡∏ô‡∏î bin ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ state)\n",
        "- **Learning rate**: ‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Q-Value\n",
        "- **Initial epsilon**: ‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Ç‡∏≠‡∏á epsilon ‡πÉ‡∏ô Œµ-greedy policy\n",
        "- **Epsilon decay rate**: ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡∏•‡∏î‡∏•‡∏á‡∏Ç‡∏≠‡∏á epsilon ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏ß‡∏•‡∏≤‡∏ú‡πà‡∏≤‡∏ô‡πÑ‡∏õ\n",
        "- **Final epsilon**: ‡∏Ñ‡πà‡∏≤ epsilon ‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏¢‡∏±‡∏á‡∏°‡∏µ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™ exploration\n",
        "- **Discount factor (Œ≥)**: ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï"
      ],
      "metadata": {
        "id": "Nm41hvefc3pn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Core Functions**"
      ],
      "metadata": {
        "id": "ARybEw_-dKoL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. `get_discretize_action()`: ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ discrete action ‡∏à‡∏≤‡∏Å policy ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Œµ-greedy\n",
        "2. `mapping_action()`: ‡πÅ‡∏õ‡∏•‡∏á discrete action ‡πÑ‡∏õ‡∏™‡∏π‡πà‡∏Ñ‡πà‡∏≤ continuous ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô `[action_min, action_max]`\n",
        "3. `discretize_state()`: ‡∏à‡∏±‡∏î‡πÅ‡∏ö‡πà‡∏á (discretize) ‡πÅ‡∏•‡∏∞ scale ‡∏Ñ‡πà‡∏≤ state ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô continuous ‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏π‡∏õ discrete ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ agent ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏á‡πà‡∏≤‡∏¢\n",
        "4. `decay_epsilon()`: ‡∏•‡∏î‡∏Ñ‡πà‡∏≤ epsilon ‡∏ï‡∏≤‡∏°‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡∏•‡∏î‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î\n",
        "5. `save_q_value()` ‡πÅ‡∏•‡∏∞ `load_q_value()`: ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å/‡πÇ‡∏´‡∏•‡∏î Q(s,a) ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö‡πÉ‡∏ô‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞ dictionary ‡∏´‡∏£‡∏∑‡∏≠ defaultdict"
      ],
      "metadata": {
        "id": "nQi4lnpKdsky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‡∏à‡∏≤‡∏Å Cart-Pole environment ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• state ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏°‡∏µ 4 components ‡∏´‡∏•‡∏±‡∏Å ‡πÜ ‡πÑ‡∏î‡πâ‡πÅ‡∏Å‡πà:\n",
        "1. **Relative joint velocity** ‡∏Ç‡∏≠‡∏á cart\n",
        "2. **Relative joint velocity** ‡∏Ç‡∏≠‡∏á pole\n",
        "3. **Relative joint position** ‡∏Ç‡∏≠‡∏á cart\n",
        "4. **Relative joint position** ‡∏Ç‡∏≠‡∏á pole\n",
        "\n",
        "‡πÅ‡∏ï‡πà‡∏•‡∏∞ component ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤ continuous ‡πÅ‡∏•‡∏∞‡∏ä‡πà‡∏ß‡∏á‡∏Ñ‡πà‡∏≤‡∏≠‡∏≤‡∏à‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏°‡∏≤‡∏Å ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á discretize ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡πà‡∏ß‡∏á (bins) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á Q-Value ‡πÑ‡∏î‡πâ‡∏™‡∏∞‡∏î‡∏ß‡∏Å"
      ],
      "metadata": {
        "id": "5x8jYYeXfKyK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï (boundary) ‡∏Ñ‡∏£‡πà‡∏≤‡∏ß ‡πÜ ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ clip:\n",
        "1. **cart position**: [-4.5, 4.5]\n",
        "2. **pole angle**: [-œÄ, œÄ]\n",
        "3. **cart velocity**: [-10, 10]\n",
        "4. **pole angular velocity**: [-10, 10]"
      ],
      "metadata": {
        "id": "-y5-Y5w1fM45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÇ‡∏Ñ‡πâ‡∏î‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô `discretize_state(self, obs)`**"
      ],
      "metadata": {
        "id": "6rhGk0cAfQdl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def discretize_state(self, obs: dict):\n",
        "    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô bins ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£\n",
        "    pose_cart_bin = self.discretize_state_weight[0]\n",
        "    pose_pole_bin = self.discretize_state_weight[1]\n",
        "    vel_cart_bin = self.discretize_state_weight[2]\n",
        "    vel_pole_bin = self.discretize_state_weight[3]\n",
        "\n",
        "    # ‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö clipping\n",
        "    pose_cart_bound = 4.5\n",
        "    pose_pole_bound = np.pi\n",
        "    vel_cart_bound = 10\n",
        "    vel_pole_bound = 10\n",
        "\n",
        "    # ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å obs['policy'] (‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô)\n",
        "    pose_cart_raw = obs['policy'][0, 0]\n",
        "    pose_pole_raw = obs['policy'][0, 1]\n",
        "    vel_cart_raw  = obs['policy'][0, 2]\n",
        "    vel_pole_raw  = obs['policy'][0, 3]\n",
        "\n",
        "    # clip ‡∏Ñ‡πà‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï\n",
        "    pose_cart_clip = torch.clip(pose_cart_raw, -pose_cart_bound, pose_cart_bound)\n",
        "    pose_pole_clip = torch.clip(pose_pole_raw, -pose_pole_bound, pose_pole_bound)\n",
        "    vel_cart_clip  = torch.clip(vel_cart_raw, -vel_cart_bound, vel_cart_bound)\n",
        "    vel_pole_clip  = torch.clip(vel_pole_raw, -vel_pole_bound, vel_pole_bound)\n",
        "\n",
        "    device = pose_cart_clip.device\n",
        "\n",
        "    # ‡∏™‡∏£‡πâ‡∏≤‡∏á grid ‡πÅ‡∏ö‡∏ö linspace ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£\n",
        "    pose_cart_grid = torch.linspace(-pose_cart_bound, pose_cart_bound, pose_cart_bin, device=device)\n",
        "    pose_pole_grid = torch.linspace(-pose_pole_bound, pose_pole_bound, pose_pole_bin, device=device)\n",
        "    vel_cart_grid  = torch.linspace(-vel_cart_bound, vel_cart_bound, vel_cart_bin, device=device)\n",
        "    vel_pole_grid  = torch.linspace(-vel_pole_bound, vel_pole_bound, vel_pole_bin, device=device)\n",
        "\n",
        "    # bucketize ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å continuous -> discrete\n",
        "    pose_cart_dig = torch.bucketize(pose_cart_clip, pose_cart_grid)\n",
        "    pose_pole_dig = torch.bucketize(pose_pole_clip, pose_pole_grid)\n",
        "    vel_cart_dig  = torch.bucketize(vel_cart_clip,  vel_cart_grid)\n",
        "    vel_pose_dig  = torch.bucketize(vel_pole_clip,  vel_pole_grid)\n",
        "\n",
        "    return (\n",
        "        int(pose_cart_dig),\n",
        "        int(pose_pole_dig),\n",
        "        int(vel_cart_dig),\n",
        "        int(vel_pose_dig)\n",
        "    )"
      ],
      "metadata": {
        "id": "dGudkj1mfTsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô:**\n",
        "- ‡∏Ñ‡∏•‡∏¥‡∏õ‡∏Ñ‡πà‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏≠‡∏≠‡∏Å‡∏ô‡∏≠‡∏Å‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï\n",
        "- ‡∏™‡∏£‡πâ‡∏≤‡∏á grid ‡∏î‡πâ‡∏ß‡∏¢ `torch.linspace()` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏ö‡πà‡∏á‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏õ‡πá‡∏ô bins\n",
        "- ‡πÉ‡∏ä‡πâ `torch.bucketize()` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏õ‡∏•‡∏á continuous value ‡πÑ‡∏õ‡πÄ‡∏õ‡πá‡∏ô discrete index\n",
        "- ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô tuple ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö state (‡πÄ‡∏ä‡πà‡∏ô `(idx_cart_pose, idx_pole_pose, idx_cart_vel, idx_pole_vel)`) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏≥‡πÑ‡∏õ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Q-Value"
      ],
      "metadata": {
        "id": "z-bfZnJHfazC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÇ‡∏Ñ‡πâ‡∏î‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô `get_discretize_action(self, obs_dis) -> int:`**"
      ],
      "metadata": {
        "id": "4GVzKiqNfdYd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‡πÄ‡∏•‡∏∑‡∏≠‡∏Å action ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Œµ-greedy policy:\n",
        "- ‡∏ñ‡πâ‡∏≤‡∏™‡∏∏‡πà‡∏° `rand` <= epsilon -> ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å action ‡πÅ‡∏ö‡∏ö‡∏™‡∏∏‡πà‡∏° (exploration)\n",
        "- ‡∏ñ‡πâ‡∏≤ `rand` > epsilon -> ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å action ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏Ñ‡πà‡∏≤‡∏ï‡∏≠‡∏ö‡πÅ‡∏ó‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å‡∏ï‡∏≤‡∏£‡∏≤‡∏á Q (exploitation)"
      ],
      "metadata": {
        "id": "x0M2N75Wfetd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_discretize_action(self, obs_dis) -> int:\n",
        "    rand = np.random.rand()\n",
        "    if rand <= self.epsilon:\n",
        "        # Exploration: ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å action ‡πÅ‡∏ö‡∏ö‡∏™‡∏∏‡πà‡∏°\n",
        "        return np.random.randint(0, self.num_of_action)\n",
        "    else:\n",
        "        # Exploitation: ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å action ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏°‡∏≤‡∏Å‡∏™‡∏∏‡∏î‡πÉ‡∏ô q_values\n",
        "        return np.argmax(self.q_values[obs_dis])"
      ],
      "metadata": {
        "id": "4iLqWNPJfgOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÇ‡∏Ñ‡πâ‡∏î‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô `mapping_action(self, action) -> torch.Tensor:`**"
      ],
      "metadata": {
        "id": "_v2TusX6fj_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‡πÅ‡∏õ‡∏•‡∏á discrete action ‡πÑ‡∏õ‡πÄ‡∏õ‡πá‡∏ô continuous value ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô `[action_min, action_max]`"
      ],
      "metadata": {
        "id": "FemHf5IqflfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mapping_action(self, action_idx):\n",
        "    \"\"\"\n",
        "    ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏Ñ‡πà‡∏≤ action ‡πÅ‡∏ö‡∏ö discrete [0..(n-1)] ‡πÑ‡∏õ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á [min, max]\n",
        "    \"\"\"\n",
        "    step_size = (self.action_range[1] - self.action_range[0]) / (self.num_of_action - 1)\n",
        "    continuous_value = self.action_range[0] + action_idx * step_size\n",
        "    return torch.tensor([[continuous_value]])  # shape ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á (1,1)"
      ],
      "metadata": {
        "id": "-yZhwk78fot7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÇ‡∏Ñ‡πâ‡∏î‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô `decay_epsilon(self):`**"
      ],
      "metadata": {
        "id": "Rq-FBimffzE4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‡∏•‡∏î‡∏Ñ‡πà‡∏≤‡∏Ç‡∏≠‡∏á epsilon ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ episode ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ agent ‡∏Ñ‡πà‡∏≠‡∏¢ ‡πÜ ‡∏•‡∏î‡∏Å‡∏≤‡∏£‡∏™‡∏∏‡πà‡∏°‡∏™‡∏≥‡∏£‡∏ß‡∏à"
      ],
      "metadata": {
        "id": "BSUiK9W2f1gw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decay_epsilon(self):\n",
        "    self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)"
      ],
      "metadata": {
        "id": "BJczaeNpf2R6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‡∏´‡∏≤‡∏Å‡πÉ‡∏ä‡πâ decay ‡∏£‡∏π‡∏õ‡∏≠‡∏∑‡πà‡∏ô (‡πÄ‡∏ä‡πà‡∏ô exponential) ‡∏Å‡πá‡∏õ‡∏£‡∏±‡∏ö logic ‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£"
      ],
      "metadata": {
        "id": "KVw9xq-zf4Dc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **save_q_value()** ‡πÅ‡∏•‡∏∞ **load_q_value()**\n",
        "‡πÉ‡∏ä‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ñ‡πà‡∏≤‡∏ï‡∏≤‡∏£‡∏≤‡∏á Q(s,a) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏†‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö offline"
      ],
      "metadata": {
        "id": "WXqTA63-f6N5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **2. Learning Algorithms**"
      ],
      "metadata": {
        "id": "t6-fBGBDf9BL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå **Algorithm** ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏•‡∏≤‡∏™‡∏à‡∏∞‡∏™‡∏∑‡∏ö‡∏ó‡∏≠‡∏î‡∏à‡∏≤‡∏Å Base Class ‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô `update()` ‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏Å‡∏ô‡∏´‡∏•‡∏±‡∏Å‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Q-Value ‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô"
      ],
      "metadata": {
        "id": "KU0hQxD6g2tm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Monte Carlo**\n",
        "- ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ñ‡πà‡∏≤ Q-value ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏à‡∏ö episode (First-visit ‡∏´‡∏£‡∏∑‡∏≠ Every-visit)\n",
        "- ‡∏™‡∏∞‡∏™‡∏° reward ‡∏Å‡∏•‡∏±‡∏ö‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á (return G) ‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÄ‡∏â‡∏û‡∏≤‡∏∞ state-action ‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏á‡πÄ‡∏à‡∏≠‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å‡πÉ‡∏ô episode ‡∏ô‡∏±‡πâ‡∏ô"
      ],
      "metadata": {
        "id": "bbf-gwblg4Sm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update(self, obs_dis, action_idx, reward, done):\n",
        "    self.obs_hist.append(obs_dis)\n",
        "    self.action_hist.append(action_idx)\n",
        "    self.reward_hist.append(reward)\n",
        "\n",
        "    if done:\n",
        "        G_cum = 0  # return ‡∏™‡∏∞‡∏™‡∏°‡∏à‡∏≤‡∏Å‡∏ó‡πâ‡∏≤‡∏¢ episode\n",
        "        for t in reversed(range(len(self.obs_hist))):\n",
        "            G_cum = self.discount_factor * G_cum + self.reward_hist[t]\n",
        "            # First-visit checking\n",
        "            if (self.obs_hist[t], self.action_hist[t]) not in list(zip(self.obs_hist[:t], self.action_hist[:t])):\n",
        "                self.n_values[self.obs_hist[t]][self.action_hist[t]] += 1\n",
        "                old_q = self.q_values[self.obs_hist[t]][self.action_hist[t]]\n",
        "                self.q_values[self.obs_hist[t]][self.action_hist[t]] = old_q + (G_cum - old_q) / \\\n",
        "                    self.n_values[self.obs_hist[t]][self.action_hist[t]]\n",
        "\n",
        "        self.obs_hist.clear()\n",
        "        self.action_hist.clear()\n",
        "        self.reward_hist.clear()"
      ],
      "metadata": {
        "id": "JLi-YozZg5eB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **SARSA**\n",
        "- ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ñ‡πà‡∏≤ Q ‡∏î‡πâ‡∏ß‡∏¢‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö on-policy\n",
        "- ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏Ç‡∏≠‡∏á `Q(s_{t+1}, a_{t+1})` ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏à‡∏≤‡∏Å policy ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô (Œµ-greedy) ‡∏°‡∏≤ bootstrap\n",
        "\n",
        "$$Q(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\alpha [R_{t+1} + \\gamma \\cdot Q(S_{t+1}, A_{t+1}) - Q(S_t,A_t)]$$"
      ],
      "metadata": {
        "id": "uTtMFWavg7Ef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update(self, obs_dis, action_idx, reward, next_obs_dis, done):\n",
        "    if done:\n",
        "        # ‡∏´‡∏≤‡∏Å‡∏à‡∏ö episode ‡πÑ‡∏°‡πà‡∏°‡∏µ next state ‡πÉ‡∏´‡πâ bootstrap\n",
        "        self.q_values[obs_dis][action_idx] += self.lr * (reward - self.q_values[obs_dis][action_idx])\n",
        "    else:\n",
        "        # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å action ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö state ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ ‡∏î‡πâ‡∏ß‡∏¢ epsilon-greedy ‡πÅ‡∏ö‡∏ö SARSA\n",
        "        next_a = self.get_discretize_action(next_obs_dis)\n",
        "        td_target = reward + self.discount_factor * self.q_values[next_obs_dis][next_a]\n",
        "        td_error = td_target - self.q_values[obs_dis][action_idx]\n",
        "        self.q_values[obs_dis][action_idx] += self.lr * td_error"
      ],
      "metadata": {
        "id": "9t1aC8TIg_Qq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q-Learning**\n",
        "- Off-policy: ‡πÉ‡∏ä‡πâ max action value ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ bootstrap ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô action ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á\n",
        "\n",
        "$$Q(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\alpha [R_{t+1} + \\gamma \\cdot \\max_{a} Q(S_{t+1}, a) - Q(S_t,A_t)]$$"
      ],
      "metadata": {
        "id": "H6rc99q8hCFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update(self, obs_dis, action_idx, next_obs_dis, reward):\n",
        "    best_next_action = np.max(self.q_values[next_obs_dis])\n",
        "    td_target = reward + self.discount_factor * best_next_action\n",
        "    td_error = td_target - self.q_values[obs_dis][action_idx]\n",
        "    self.q_values[obs_dis][action_idx] += self.lr * td_error"
      ],
      "metadata": {
        "id": "eu92g8SThDWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Double Q-Learning**\n",
        "- ‡πÉ‡∏ä‡πâ Q-table ‡∏™‡∏≠‡∏á‡∏ï‡∏±‡∏ß (QA ‡πÅ‡∏•‡∏∞ QB) ‡∏™‡∏•‡∏±‡∏ö‡∏Å‡∏±‡∏ô‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏£‡πÄ‡∏ó‡∏≤‡∏Å‡∏≤‡∏£ overestimation ‡∏Ç‡∏≠‡∏á Q-Learning\n",
        "- ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å action ‡∏à‡∏≤‡∏Å QA ‡πÅ‡∏ï‡πà‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏î‡πâ‡∏ß‡∏¢ QB ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏•‡∏∑‡∏≠‡∏Å action ‡∏à‡∏≤‡∏Å QB ‡πÅ‡∏ï‡πà‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏î‡πâ‡∏ß‡∏¢ QA"
      ],
      "metadata": {
        "id": "2XHYODK6hGPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update(self, obs_dis, action_idx, next_obs_dis, reward):\n",
        "    if np.random.rand() < 0.5:\n",
        "        a_star = np.argmax(self.qa_values[next_obs_dis])\n",
        "        td_target = reward + self.discount_factor * self.qb_values[next_obs_dis][a_star]\n",
        "        td_error = td_target - self.qa_values[obs_dis][action_idx]\n",
        "        self.qa_values[obs_dis][action_idx] += self.lr * td_error\n",
        "    else:\n",
        "        b_star = np.argmax(self.qb_values[next_obs_dis])\n",
        "        td_target = reward + self.discount_factor * self.qa_values[next_obs_dis][b_star]\n",
        "        td_error = td_target - self.qb_values[obs_dis][action_idx]\n",
        "        self.qb_values[obs_dis][action_idx] += self.lr * td_error\n",
        "\n",
        "    # ‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ q_values ‡∏´‡∏•‡∏±‡∏Å‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö‡∏ú‡∏•‡∏£‡∏ß‡∏°‡∏à‡∏≤‡∏Å QA/QB ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÉ‡∏î‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏´‡∏ô‡∏∂‡πà‡∏á\n",
        "    self.q_values[obs_dis][action_idx] = self.qa_values[obs_dis][action_idx]"
      ],
      "metadata": {
        "id": "SCniFREXhIjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‡πÇ‡∏î‡∏¢‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î‡∏Ñ‡∏∑‡∏≠ ‡πÉ‡∏´‡πâ QA ‡πÅ‡∏•‡∏∞ QB ‡πÅ‡∏ö‡πà‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏Å‡∏±‡∏ô‡∏Ñ‡∏ô‡∏•‡∏∞‡∏£‡∏≠‡∏ö ‡∏à‡∏∂‡∏á‡∏ä‡πà‡∏ß‡∏¢‡∏•‡∏î‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡πà‡∏≤‡∏™‡∏π‡∏á‡πÄ‡∏Å‡∏¥‡∏ô‡∏à‡∏£‡∏¥‡∏á (overestimation) ‡∏ó‡∏µ‡πà‡∏°‡∏±‡∏Å‡πÄ‡∏Å‡∏¥‡∏î‡πÉ‡∏ô Q-Learning"
      ],
      "metadata": {
        "id": "UYHZvIKQhLEJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "juhcXv-OjljI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 2: Training & Tuning to Stabilize Cart-Pole Agent**\n",
        "\n",
        "## ‡πÉ‡∏ô Part ‡∏ô‡∏µ‡πâ ‡πÄ‡∏£‡∏≤‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏à‡∏π‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏±‡πâ‡∏á 4 Model ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤ Hyperparameter ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°\n",
        "\n",
        "1.1 ‡πÇ‡∏î‡∏¢‡πÄ‡∏£‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏à‡∏π‡∏ô hyperparameter ‡∏Ç‡∏≠‡∏á Q-Learning ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô Base model\n",
        "‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å Q-Learning ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤‡∏£‡∏∏‡πà‡∏ô‡∏≠‡∏∑‡πà‡∏ô ‡πÅ‡∏•‡∏∞‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÑ‡∏î‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô\n",
        "\n",
        "1.2 ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡πÑ‡∏î‡πâ‡∏ä‡∏∏‡∏î parameter ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ performance ‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å Q-Learning ‡πÅ‡∏•‡πâ‡∏ß\n",
        "‡πÄ‡∏£‡∏≤‡∏à‡∏∂‡∏á‡∏ô‡∏≥‡∏ä‡∏∏‡∏î parameter ‡πÄ‡∏î‡∏¥‡∏°‡πÑ‡∏õ‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏Å‡∏±‡∏ö DQL, SARSA ‡πÅ‡∏•‡∏∞ MC ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ß‡πà‡∏≤\n",
        "‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà ‡∏†‡∏≤‡∏¢‡πÉ‡∏ï‡πâ‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô\n",
        "\n",
        "1.3 ‡∏ô‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏ô‡∏µ‡πâ ‡πÄ‡∏£‡∏≤‡∏¢‡∏±‡∏á‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°-‡∏•‡∏î‡∏Ñ‡πà‡∏≤‡∏Ç‡∏≠‡∏á hyperparameter ‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ö‡∏≤‡∏á‡∏ï‡∏±‡∏ß\n",
        "‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏ß (sensitive) ‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á parameter ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á"
      ],
      "metadata": {
        "id": "BJIQ-LGPjl7T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô! ‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡∏Ñ‡∏∑‡∏≠‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á **hyperparameters** ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏≠‡∏¢‡∏π‡πà ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡πÉ‡∏ô **Part 2** ‡∏Ç‡∏≠‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô (Training & Tuning) ‡πÄ‡∏£‡∏≤‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡πÑ‡∏´‡∏ô‡∏ö‡πâ‡∏≤‡∏á ‚Äî ‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö Bullet Point ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡πÉ‡∏ô Jupyter Notebook:\n",
        "\n",
        "---\n",
        "\n",
        "## üîß Hyperparameter Overview (Used in Part 2)\n",
        "\n",
        "- `num_of_action`\n",
        "  - ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Ç‡∏≠‡∏á action space (‡∏à‡∏≥‡∏ô‡∏ß‡∏ô action ‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏î‡πâ)\n",
        "  - ‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏ô **Run 3 (Action/Observation space test)** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏ú‡∏•‡∏ï‡πà‡∏≠ learning efficiency\n",
        "\n",
        "- `action_range`\n",
        "  - ‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï‡∏Ñ‡πà‡∏≤‡∏Ç‡∏≠‡∏á action (‡πÄ‡∏ä‡πà‡∏ô ‡πÅ‡∏£‡∏á‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏£‡∏ñ‡πÄ‡∏Ç‡πá‡∏ô)\n",
        "  - **‡πÑ‡∏°‡πà‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏ô‡∏£‡∏≠‡∏ö‡∏ô‡∏µ‡πâ** (‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏á‡∏ó‡∏µ‡πà‡∏à‡∏≤‡∏Å base parameter)\n",
        "\n",
        "- `discretize_state_weight`\n",
        "  - ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á observation (continuous state) ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô discrete state\n",
        "  - ‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏ô **Run 3 (Action/Observation space test)** ‡∏Ñ‡∏π‡πà‡∏Å‡∏±‡∏ö `num_of_action`\n",
        "\n",
        "- `learning_rate`\n",
        "  - ‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Q-values\n",
        "  - ‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏ô **Run 1 (Min LR)** ‡πÅ‡∏•‡∏∞ **Run 2 (Max LR)** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤ learning efficiency ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n",
        "\n",
        "- `n_episodes`\n",
        "  - ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ run\n",
        "  - ‡πÉ‡∏ä‡πâ `5,000` ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö tuning ‡∏ó‡∏∏‡∏Å run  \n",
        "  - ‡πÉ‡∏ä‡πâ `20,000` ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö final run ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
        "\n",
        "- `start_epsilon`\n",
        "  - ‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Ç‡∏≠‡∏á epsilon ‡πÉ‡∏ô epsilon-greedy policy (‡πÉ‡∏ä‡πâ‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏Å‡∏≤‡∏£ explore)\n",
        "  - ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏á‡∏ó‡∏µ‡πà `1.0` ‡∏ï‡∏•‡∏≠‡∏î‡∏ó‡∏∏‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á\n",
        "\n",
        "- `epsilon_decay`\n",
        "  - ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡∏•‡∏î‡∏•‡∏á‡∏Ç‡∏≠‡∏á epsilon ‡∏ï‡πà‡∏≠ episode\n",
        "  - ‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏ô **Run 4 (Min ED)** ‡πÅ‡∏•‡∏∞ **Run 5 (Max ED)**\n",
        "\n",
        "- `final_epsilon`\n",
        "  - ‡∏Ñ‡πà‡∏≤‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á epsilon (‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£ explore)\n",
        "  - **‡πÑ‡∏°‡πà‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏ô‡∏£‡∏≠‡∏ö‡∏ô‡∏µ‡πâ** (‡∏Ñ‡∏á‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà `0.05`)\n",
        "\n",
        "- `discount`\n",
        "  - discount factor ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö future reward\n",
        "  - **‡πÑ‡∏°‡πà‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏ô‡∏£‡∏≠‡∏ö‡∏ô‡∏µ‡πâ** (‡∏Ñ‡∏á‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà `1`)\n",
        "\n",
        "---\n",
        "\n",
        "‡∏´‡∏≤‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡πà‡∏≠‡πÉ‡∏ô cell ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ ‡∏Å‡πá‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏¢‡∏Å‡πÉ‡∏™‡πà‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ run ‡∏ï‡πà‡∏≠‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö."
      ],
      "metadata": {
        "id": "mwBMJr_NUlHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameter ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ Train Agent\n",
        "num_of_action = 5\n",
        "action_range = [-12, 12]  # ‡∏ä‡πà‡∏ß‡∏á‡∏Ç‡∏≠‡∏á action\n",
        "discretize_state_weight = [4, 8, 4, 4]  # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô bins ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ state: [pose_cart, pose_pole, vel_cart, vel_pole]\n",
        "learning_rate = 0.03\n",
        "n_episodes = 5000\n",
        "start_epsilon = 1.0\n",
        "epsilon_decay = 0.00003  # ‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏≥‡∏£‡∏ß‡∏à\n",
        "final_epsilon = 0.05\n",
        "discount = 1\n",
        "\n",
        "# Parameter ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö clipping ‡∏Ç‡∏≠‡∏á state values\n",
        "pose_cart_bound = 3\n",
        "pose_pole_bound = float(np.deg2rad(24.0))\n",
        "vel_cart_bound = 15\n",
        "vel_pole_bound = 15"
      ],
      "metadata": {
        "id": "o6s_cboejuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á State Space**\n",
        "\n",
        "‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ agent ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏°‡∏ä‡∏°‡∏ó‡∏∏‡∏Å state ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ ‡πÄ‡∏£‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏Å‡∏≥‡∏´‡∏ô‡∏î state space ‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏•‡πá‡∏Å ‡πÇ‡∏î‡∏¢‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÑ‡∏î‡πâ‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ:\n",
        "\n",
        "$$N_S = S_{pose\\_cart} \\times S_{pose\\_pole} \\times S_{vel\\_cart} \\times S_{vel\\_pole}$$\n",
        "\n",
        "‡πÇ‡∏î‡∏¢‡∏ó‡∏µ‡πà‡∏à‡∏≥‡∏ô‡∏ß‡∏ô state (bins) ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡∏ñ‡∏π‡∏Å‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÇ‡∏î‡∏¢ `discretize_state_weight` ‡πÄ‡∏ä‡πà‡∏ô ‡πÉ‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πâ:\n",
        "\n",
        "$$N_S = 4 \\times 8 \\times 4 \\times 4 = 512$$\n",
        "\n",
        "‡πÅ‡∏•‡∏∞‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á Q-table ‡∏Ñ‡∏∑‡∏≠:\n",
        "\n",
        "$$N_Q = N_S \\times N_A = 512 \\times 5 = 2560$$\n",
        "\n",
        "‡∏ã‡∏∂‡πà‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ß‡πà‡∏≤‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ episode agent ‡∏°‡∏µ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏°‡∏ä‡∏° state-action pair ‡πÑ‡∏î‡πâ‡∏ö‡πà‡∏≠‡∏¢‡∏Ñ‡∏£‡∏±‡πâ‡∏á"
      ],
      "metadata": {
        "id": "T5vLIhvkjzv_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **‡∏Å‡∏≤‡∏£ Train & ‡∏Å‡∏≤‡∏£ Play**\n",
        "\n",
        "‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å (Train) Agent ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÉ‡∏ô terminal ‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ script ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö (Play) ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ:\n",
        "\n",
        "```bash\n",
        "# ‡∏ù‡∏∂‡∏Å Agent ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Stabilizing Cart-Pole Task\n",
        "python scripts/RL_Algorithm/train.py --task Stabilize-Isaac-Cartpole-v0\n",
        "\n",
        "# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Agent ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏ù‡∏∂‡∏Å‡πÄ‡∏™‡∏£‡πá‡∏à\n",
        "python scripts/RL_Algorithm/play.py --task Stabilize-Isaac-Cartpole-v0\n",
        "```\n",
        "\n",
        "‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡πÉ‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏ó‡∏≥‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á Stabilizing Cart-Pole Task ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏° Swing-up"
      ],
      "metadata": {
        "id": "qzhiTrU6j1h0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Experiment Runs Parameters Summary**\n",
        "\n",
        "---\n",
        "\n",
        "###  **1. QL Tuning (5K Ep. Each)**  \n",
        "#### **Run 1:** 5K QL - Min LR & Min Epsilon Decay\n",
        "```python\n",
        "learning_rate = 0.01\n",
        "epsilon_decay = 0.00001\n",
        "n_episodes = 5000\n",
        "```\n",
        "\n",
        "#### **Run 2:** 5K QL - Max LR & Max Epsilon Decay\n",
        "```python\n",
        "learning_rate = 0.1\n",
        "epsilon_decay = 0.0001\n",
        "n_episodes = 5000\n",
        "```\n",
        "\n",
        "#### **Run 3:** 5K QL - Action/Obs Space Test **‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡πÑ‡∏ß‡πâ\n",
        "```python\n",
        "num_of_action = 3\n",
        "discretize_state_weight = [3, 6, 3, 3]\n",
        "n_episodes = 5000\n",
        "```\n"
      ],
      "metadata": {
        "id": "PPhAAifGaEWY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sy7unGZfai97"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **2. Model Comparison (5K Ep. Each)**  \n",
        "\n",
        "#### **Run 4:** 5K QL - Final/Base\n",
        "```python\n",
        "learning_rate = 0.03\n",
        "epsilon_decay = 0.00003\n",
        "num_of_action = 5\n",
        "discretize_state_weight = [4, 8, 4, 4]\n",
        "n_episodes = 5000\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### **Run 5:** 5K DQL - Final/Base  \n",
        "```python\n",
        "learning_rate = 0.03\n",
        "epsilon_decay = 0.00003\n",
        "num_of_action = 5\n",
        "discretize_state_weight = [4, 8, 4, 4]\n",
        "n_episodes = 5000\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "#### **Run 7:** 5K MC - Final/Base   \n",
        "```python\n",
        "learning_rate = 0.03\n",
        "epsilon_decay = 0.00003\n",
        "num_of_action = 5\n",
        "discretize_state_weight = [4, 8, 4, 4]\n",
        "n_episodes = 5000\n",
        "```\n",
        "\n",
        "#### **Run 8:** 5K SARSA - Final/Base\n",
        "```python\n",
        "learning_rate = 0.03\n",
        "epsilon_decay = 0.00003\n",
        "num_of_action = 5\n",
        "discretize_state_weight = [4, 8, 4, 4]\n",
        "n_episodes = 5000\n",
        "```\n",
        "\n",
        "\n",
        "#### **Run 6:** 5K DQL - Lower LR\n",
        "```python\n",
        "learning_rate = 0.01\n",
        "epsilon_decay = 0.00003\n",
        "num_of_action = 5\n",
        "discretize_state_weight = [4, 8, 4, 4]\n",
        "n_episodes = 5000\n",
        "```\n",
        "\n",
        "#### **Run 9:** 5K SARSA - Lower Epsilon Decay   \n",
        "```python\n",
        "learning_rate = 0.03\n",
        "epsilon_decay = 0.00001\n",
        "num_of_action = 5\n",
        "discretize_state_weight = [4, 8, 4, 4]\n",
        "n_episodes = 5000\n",
        "```\n",
        "\n",
        "--"
      ],
      "metadata": {
        "id": "fciFOXZ0ahsT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£ Train**\n",
        "\n",
        "‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏∞‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡∏≠‡∏á Reward ‡∏ï‡πà‡∏≠ Episode ‡πÅ‡∏•‡∏∞ Q-Value Distribution ‡∏ã‡∏∂‡πà‡∏á‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ plot ‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£ Evaluate (Part 3)\n",
        "\n",
        "‡∏ô‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏ô‡∏µ‡πâ ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏±‡∏ö‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡πà‡∏≤‡∏á ‡πÜ ‡πÇ‡∏î‡∏¢‡∏°‡∏µ‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ:\n",
        "\n",
        "### **Plan ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ Tune ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö**\n",
        "\n",
        "- **1. Tune Q-Learning (QL):**\n",
        "  - ‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏Ñ‡πà‡∏≤ `learning_rate` ‡πÅ‡∏•‡∏∞ `epsilon_decay` (Min, Max, Mid) ‡∏ó‡∏µ‡∏•‡∏∞ 5K episode\n",
        "\n",
        "- **2. ‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏≠‡∏∑‡πà‡∏ô (‡πÉ‡∏ä‡πâ‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏à‡∏≤‡∏Å QL):**\n",
        "  - **Double Q-Learning (DQL):** 5K episode (‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏° ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏•‡∏î `learning_rate`)\n",
        "  - **SARSA:** 5K episode (‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏° ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏•‡∏î `epsilon_decay`)\n",
        "  - **Monte Carlo (MC):** 5K episode (‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤ `discount` ‡πÉ‡∏´‡πâ‡∏™‡∏π‡∏á‡∏Ç‡∏∂‡πâ‡∏ô ‡πÄ‡∏ä‡πà‡∏ô 0.95 ‡∏´‡∏£‡∏∑‡∏≠ 1)\n",
        "\n",
        "\n",
        "‡∏™‡∏£‡∏∏‡∏õ‡∏Ñ‡πà‡∏≤ parameter ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ run (Run Parameters Summary) ‡∏à‡∏∞‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢:\n",
        "\n",
        " **Run 1: QL - ‡∏Ñ‡πà‡∏≤ LR ‡∏ï‡πà‡∏≥‡πÅ‡∏•‡∏∞ epsilon decay ‡∏ï‡πà‡∏≥**\n",
        "\n",
        " **Run 2: QL - ‡∏Ñ‡πà‡∏≤ LR ‡∏™‡∏π‡∏á‡πÅ‡∏•‡∏∞ epsilon decay ‡∏™‡∏π‡∏á**\n",
        "\n",
        " **Run 3: QL - ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á state/action space**\n",
        "\n",
        " **Run 4: QL - ‡∏Ñ‡πà‡∏≤‡πÄ‡∏ö‡∏™‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö**\n",
        "\n",
        "‡πÅ‡∏•‡∏∞‡∏ï‡πà‡∏≠‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏≠‡∏∑‡πà‡∏ô ‡πÜ (DQL, SARSA, MC) ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡πÑ‡∏ß‡πâ"
      ],
      "metadata": {
        "id": "xd36zGIQj3PG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Q-Learning Tuning (Run 1-3)**  \n",
        "‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö hyperparameter ‡∏Ç‡∏≠‡∏á Q-Learning ‡∏û‡∏ö‡∏ß‡πà‡∏≤:\n",
        "\n",
        "- **Run 1 (Min LR & Min Decay)**  \n",
        "  ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Ñ‡πà‡∏≠‡∏ô‡∏Ç‡πâ‡∏≤‡∏á‡∏ä‡πâ‡∏≤ agent ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏ô‡∏≤‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÑ‡∏î‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏î‡∏µ ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å learning rate ‡∏ï‡πà‡∏≥‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ‡∏ó‡∏≥‡πÉ‡∏´‡πâ update ‡∏ó‡∏µ‡∏•‡∏∞‡∏ô‡πâ‡∏≠‡∏¢ ‡πÅ‡∏•‡∏∞ epsilon decay ‡∏ä‡πâ‡∏≤ agent ‡πÄ‡∏•‡∏¢‡∏¢‡∏±‡∏á‡∏™‡∏∏‡πà‡∏°‡∏ô‡∏≤‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô  \n",
        "  > ‡∏ú‡∏•‡∏Ñ‡∏∑‡∏≠ agent ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà converge ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô 5,000 episodes\n",
        "\n",
        "- **Run 2 (Max LR & Max Decay)**  \n",
        "  ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡πÅ‡∏£‡∏Å ‡πÅ‡∏ï‡πà‡∏Å‡∏£‡∏ì‡∏µ‡∏ô‡∏µ‡πâ‡∏Ñ‡πà‡∏≤ epsilon ‡∏•‡∏î‡∏•‡∏á‡πÄ‡∏£‡πá‡∏ß‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ agent ‡∏´‡∏¢‡∏∏‡∏î‡∏™‡∏∏‡πà‡∏°‡πÄ‡∏£‡πá‡∏ß ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏ï‡∏¥‡∏î local optimum  \n",
        "  > agent ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏î‡∏µ \"‡πÉ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏±‡πâ‡∏ô\" ‡πÑ‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ\n",
        "\n",
        "- **Run 3 (Lower Res in Action/Obs Space)**  \n",
        "  ‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏•‡∏á ‡∏™‡πà‡∏á‡∏ú‡∏•‡πÉ‡∏´‡πâ agent ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• input ‡∏ó‡∏µ‡πà‡∏á‡πà‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô (state space ‡∏ô‡πâ‡∏≠‡∏¢‡∏•‡∏á) ‡πÅ‡∏•‡∏∞ action ‡∏ô‡πâ‡∏≠‡∏¢‡∏•‡∏á  \n",
        "  > agent ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏° ‡πÅ‡∏ï‡πà‡∏≠‡∏≤‡∏à‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡πÑ‡∏î‡πâ‡πÑ‡∏°‡πà‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏ó‡πà‡∏≤‡∏Ñ‡πà‡∏≤‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô\n",
        "\n",
        "**Insight:**  \n",
        "‡∏Ñ‡πà‡∏≤ base case ‡πÄ‡∏î‡∏¥‡∏° (Run 4) ‡∏à‡∏∂‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏∏‡∏î‡∏™‡∏°‡∏î‡∏∏‡∏•‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏° ‡∏ã‡∏∂‡πà‡∏á‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡πÄ‡∏ä‡∏¥‡∏á performance ‡πÅ‡∏•‡∏∞ stability\n",
        "\n",
        "---\n",
        "\n",
        "###  **2. Final Q-Learning vs Other Models (Run 4-8)**  \n",
        "‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö Q-Learning ‡∏Å‡∏±‡∏ö DQL, SARSA ‡πÅ‡∏•‡∏∞ MC ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô:\n",
        "\n",
        "- **Q-Learning (Run 4)**  \n",
        "  ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏£‡πá‡∏ß‡πÅ‡∏•‡∏∞‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£ ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö environment ‡πÅ‡∏ö‡∏ö deterministic ‡∏≠‡∏¢‡πà‡∏≤‡∏á CartPole  \n",
        "\n",
        "- **DQL (Run 5)**  \n",
        "  ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á QL ‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡∏≤‡∏à‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡πÉ‡∏ô‡∏ö‡∏≤‡∏á‡∏ä‡πà‡∏ß‡∏á ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏•‡∏î bias ‡∏à‡∏≤‡∏Å max Q ‡πÉ‡∏ô Q-table ‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ Q ‡∏™‡∏≠‡∏á‡∏ï‡∏±‡∏ß  \n",
        "\n",
        "- **SARSA (Run 8)**  \n",
        "  ‡∏£‡∏∞‡∏°‡∏±‡∏î‡∏£‡∏∞‡∏ß‡∏±‡∏á‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ QL ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏≠‡∏¥‡∏á action ‡∏à‡∏£‡∏¥‡∏á‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏Å‡∏¥‡∏î‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï  \n",
        "  > ‡∏ú‡∏•‡∏Ñ‡∏∑‡∏≠‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ä‡πâ‡∏≤‡∏Å‡∏ß‡πà‡∏≤ QL ‡πÅ‡∏ï‡πà‡∏°‡∏µ‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡πÄ‡∏•‡πà‡∏ô‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏Å‡∏ß‡πà‡∏≤  \n",
        "\n",
        "- **MC (Run 7)**  \n",
        "  ‡∏Ñ‡πà‡∏≠‡∏ô‡∏Ç‡πâ‡∏≤‡∏á‡∏ä‡πâ‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£ ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏≠ episode ‡∏à‡∏ö‡∏Å‡πà‡∏≠‡∏ô‡∏ñ‡∏∂‡∏á‡∏à‡∏∞ update ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• update ‡∏ä‡πâ‡∏≤  \n",
        "  > ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö environment ‡∏ó‡∏µ‡πà episode ‡∏™‡∏±‡πâ‡∏ô‡πÅ‡∏•‡∏∞ variance ‡πÑ‡∏°‡πà‡πÄ‡∏¢‡∏≠‡∏∞‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô\n",
        "\n",
        "---\n",
        "\n",
        "###  **3. Tweaked Runs (Run 6 & 9)**  \n",
        "‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÉ‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏≠‡∏∑‡πà‡∏ô:\n",
        "\n",
        "- **Run 6: DQL with Lower LR**  \n",
        "  ‡∏õ‡∏£‡∏±‡∏ö learning rate ‡πÉ‡∏´‡πâ‡∏ï‡πà‡∏≥‡∏•‡∏á ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Q ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô  \n",
        "  > ‡∏™‡πà‡∏á‡∏ú‡∏•‡πÉ‡∏´‡πâ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡∏Ç‡∏∂‡πâ‡∏ô ‡πÅ‡∏ï‡πà‡∏ô‡πà‡∏≤‡∏à‡∏∞‡∏ä‡πâ‡∏≤‡∏Å‡∏ß‡πà‡∏≤ base case ‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢\n",
        "\n",
        "- **Run 9: SARSA with Lower Epsilon Decay**  \n",
        "  ‡∏ä‡∏∞‡∏•‡∏≠‡∏Å‡∏≤‡∏£‡∏•‡∏î epsilon ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ agent explore ‡πÑ‡∏î‡πâ‡∏ô‡∏≤‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô  \n",
        "  > ‡∏≠‡∏≤‡∏à‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤ ‡πÅ‡∏ï‡πà‡∏Å‡πá‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ä‡πâ‡∏≤‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà converge ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô 5K episodes\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "77pIGWeOb68_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bYT2qIWgV8PJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÇ‡∏Ñ‡πâ‡∏î‡∏™‡∏£‡∏∏‡∏õ Parameter ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Run ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**\n"
      ],
      "metadata": {
        "id": "Xxw3i0Puj9GS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameter ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Run 4 (QL ‡πÄ‡∏ö‡∏™)\n",
        "num_of_action = 5\n",
        "action_range = [-12, 12]\n",
        "discretize_state_weight = [4, 8, 4, 4]\n",
        "learning_rate = 0.03\n",
        "n_episodes = 5000\n",
        "start_epsilon = 1.0\n",
        "epsilon_decay = 0.00003\n",
        "final_epsilon = 0.05\n",
        "discount = 1\n",
        "\n",
        "# Parameter ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ clipping ‡∏Ç‡∏≠‡∏á state\n",
        "pose_cart_bound = 3\n",
        "pose_pole_bound = float(np.deg2rad(24.0))\n",
        "vel_cart_bound = 15\n",
        "vel_pole_bound = 15"
      ],
      "metadata": {
        "id": "Z9MR3hzJooVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Summary ‡∏Ç‡∏≠‡∏á Part 2**\n",
        "\n",
        "- ‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ ‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ Train ‡πÅ‡∏•‡∏∞ Play Agent ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Stabilizing Cart-Pole\n",
        "- ‡πÑ‡∏î‡πâ‡∏™‡∏£‡∏∏‡∏õ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏ó‡∏±‡πâ‡∏á‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤ (tuning) ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡πà‡∏≤‡∏á ‡πÜ\n",
        "- ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ç‡∏ô‡∏≤‡∏î state space ‡πÅ‡∏•‡∏∞ Q-table ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏°‡∏ä‡∏° state\n",
        "- ‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô Play ‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á terminal ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏ô script ‡∏ó‡∏î‡∏™‡∏≠‡∏ö ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå‡∏Å‡∏≥‡∏´‡∏ô‡∏î\n",
        "\n",
        "‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ô‡∏≥‡πÑ‡∏õ‡∏™‡∏π‡πà‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ú‡∏• (Evaluation) ‡πÉ‡∏ô Part 3"
      ],
      "metadata": {
        "id": "iFXpxRVKkAiE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "i_UjXsmdnl09"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 3: Evaluate `Cart-Pole` Agent Performance.**"
      ],
      "metadata": {
        "id": "58-229rKnmvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import *"
      ],
      "metadata": {
        "id": "oVPF4JMAMC5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Q‚Äëtable ‡∏ô‡∏±‡πâ‡∏ô‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡πâ‡∏≤‡∏ó‡∏≤‡∏¢ ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å state space ‡∏°‡∏µ‡∏°‡∏¥‡∏ï‡∏¥‡∏™‡∏π‡∏á ‡πÉ‡∏ô‡∏Å‡∏£‡∏≤‡∏ü 3D ‡πÅ‡∏Å‡∏ô z ‡∏°‡∏±‡∏Å‡∏à‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡πà‡∏≤ maximum action‚Äëvalue ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ state ‡∏ã‡∏∂‡πà‡∏á‡∏ö‡πà‡∏á‡∏ö‡∏≠‡∏Å‡∏ñ‡∏∂‡∏á‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û (expected return) ‡∏Ç‡∏≠‡∏á state ‡∏ô‡∏±‡πâ‡∏ô ‡πÜ"
      ],
      "metadata": {
        "id": "3aB6-3qWEgUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Baseline Experiment**\n",
        "‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ ‡πÄ‡∏£‡∏≤‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏• performance ‡∏Ç‡∏≠‡∏á‡∏ó‡∏∏‡∏Å algorithm ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á base‚Äëline ‡∏ó‡∏µ‡πà‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô (‡∏ï‡∏≤‡∏° Part¬†2)"
      ],
      "metadata": {
        "id": "fAGvMUCsEhlr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q-Learning**\n",
        "‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÑ‡∏ü‡∏•‡πå reward ‡πÅ‡∏•‡∏∞ Q‚Äëvalue ‡∏ó‡∏µ‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ß‡πâ:"
      ],
      "metadata": {
        "id": "2BQ6PekfEi7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_learn_reward_file = \"reward_value/QL_r_0.json\"\n",
        "q_learn_q_value_file_1000 = \"q_value/Stabilize/Q_Learning/Q_Learning0/Q_Learning_0_1000_5_12_4_8.json\"\n",
        "q_learn_q_value_file_3000 = \"q_value/Stabilize/Q_Learning/Q_Learning0/Q_Learning_0_3000_5_12_4_8.json\"\n",
        "q_learn_q_value_file_5000 = \"q_value/Stabilize/Q_Learning/Q_Learning0/Q_Learning_0_5000_5_12_4_8.json\"\n",
        "\n",
        "plot_reward(q_learn_reward_file)\n",
        "plot_reward_grouped(q_learn_reward_file, 100)\n",
        "\n",
        "plot_q_values_3d_dual(load_q_values(q_learn_q_value_file_1000), (0,1), (4,8), (2,3), (4,4))\n",
        "plot_q_values_3d_dual(load_q_values(q_learn_q_value_file_3000), (0,1), (4,8), (2,3), (4,4))\n",
        "plot_q_values_3d_dual(load_q_values(q_learn_q_value_file_5000), (0,1), (4,8), (2,3), (4,4))"
      ],
      "metadata": {
        "id": "7bzIADl9Ek-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Learning Efficiency (Q-Learning):**\n",
        "- ‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á 1000 episode, agent ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Q‚Äëvalue ‡πÄ‡∏°‡∏∑‡πà‡∏≠ cart ‡πÅ‡∏•‡∏∞ pole ‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà‡∏®‡∏π‡∏ô‡∏¢‡πå‡∏Å‡∏•‡∏≤‡∏á ‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏Ç‡∏≠‡∏á cart ‡∏™‡πà‡∏á‡∏ú‡∏•‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏™‡∏ß‡∏¥‡∏á‡∏Ç‡∏≠‡∏á pole\n",
        "- ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ù‡∏∂‡∏Å‡πÑ‡∏õ‡∏ñ‡∏∂‡∏á 5000 episode ‡∏û‡∏ö‡∏ß‡πà‡∏≤ agent ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å‡∏Å‡∏±‡∏ö state ‡∏ó‡∏µ‡πà‡∏°‡∏µ reward ‡∏™‡∏π‡∏á ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏° cart ‡πÉ‡∏´‡πâ‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤‡∏ó‡∏µ‡πà‡∏®‡∏π‡∏ô‡∏¢‡πå‡∏Å‡∏•‡∏≤‡∏á"
      ],
      "metadata": {
        "id": "pL9_dE7KEn7N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deployment Performance (Q-Learning):**\n",
        "- Agent ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ñ‡∏á pole ‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏™‡∏±‡πâ‡∏ô ‡πÜ ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡πÉ‡∏´‡πâ cart ‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤‡∏ó‡∏µ‡πà‡∏®‡∏π‡∏ô‡∏¢‡πå‡∏Å‡∏•‡∏≤‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á"
      ],
      "metadata": {
        "id": "y7BqNxcWEpPa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Double Q-Learning**\n",
        "‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÑ‡∏ü‡∏•‡πå reward ‡πÅ‡∏•‡∏∞ Q‚Äëvalue:"
      ],
      "metadata": {
        "id": "RPFmFrJuEqQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dq_learn_reward_file = \"reward_value/Double_Q_Learning_r_0.json\"\n",
        "dq_learn_q_value_file_1000 = \"q_value/Stabilize/Double_Q_Learning/Double_Q_Learning0/Double_Q_Learning_0_1000_5_12_4_8.json\"\n",
        "dq_learn_q_value_file_3000 = \"q_value/Stabilize/Double_Q_Learning/Double_Q_Learning0/Double_Q_Learning_0_3000_5_12_4_8.json\"\n",
        "dq_learn_q_value_file_5000 = \"q_value/Stabilize/Double_Q_Learning/Double_Q_Learning0/Double_Q_Learning_0_5000_5_12_4_8.json\"\n",
        "\n",
        "plot_reward_grouped(dq_learn_reward_file, 100)\n",
        "plot_q_values_3d_dual(load_q_values(dq_learn_q_value_file_1000), (0,1), (4,8), (2,3), (4,4))\n",
        "plot_q_values_3d_dual(load_q_values(dq_learn_q_value_file_3000), (0,1), (4,8), (2,3), (4,4))\n",
        "plot_q_values_3d_dual(load_q_values(dq_learn_q_value_file_5000), (0,1), (4,8), (2,3), (4,4))"
      ],
      "metadata": {
        "id": "Qp5rzwzNErx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Learning Efficiency (Double Q-Learning):**\n",
        "- ‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡πÅ‡∏£‡∏Å agent ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á Q‚Äëvalue ‡πÉ‡∏ô‡∏ö‡∏≤‡∏á state ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡πÉ‡∏ô‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡πà‡∏≤ ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÉ‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£ termination\n",
        "**Deployment Performance (Double Q-Learning):**\n",
        "- ‡∏ú‡∏•‡∏Å‡∏≤‡∏£ deploy ‡πÅ‡∏™‡∏î‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤ Double Q‚ÄëLearning ‡∏•‡∏î bias ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡πà‡∏≤‡∏™‡∏π‡∏á‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ‡∏ö‡πâ‡∏≤‡∏á ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏° cart"
      ],
      "metadata": {
        "id": "_4I97ADCEtTh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **SARSA**\n",
        "‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÑ‡∏ü‡∏•‡πå reward ‡πÅ‡∏•‡∏∞ Q‚Äëvalue:"
      ],
      "metadata": {
        "id": "3gE8SrefEutO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SARSA_reward_file = \"reward_value/SARSA_r_0.json\"\n",
        "SARSA_q_value_file_1000 = \"q_value/Stabilize/SARSA/SARSA0/SARSA_0_1000_5_12_4_8.json\"\n",
        "SARSA_q_value_file_3000 = \"q_value/Stabilize/SARSA/SARSA0/SARSA_0_3000_5_12_4_8.json\"\n",
        "SARSA_q_value_file_5000 = \"q_value/Stabilize/SARSA/SARSA0/SARSA_0_5000_5_12_4_8.json\"\n",
        "\n",
        "plot_reward_grouped(SARSA_reward_file, 100)\n",
        "plot_q_values_3d_dual(load_q_values(SARSA_q_value_file_1000), (0,1), (4,8), (2,3), (4,4))\n",
        "plot_q_values_3d_dual(load_q_values(SARSA_q_value_file_3000), (0,1), (4,8), (2,3), (4,4))\n",
        "plot_q_values_3d_dual(load_q_values(SARSA_q_value_file_5000), (0,1), (4,8), (2,3), (4,4))"
      ],
      "metadata": {
        "id": "n_fgFF_oEvy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Learning Efficiency (SARSA):**\n",
        "- SARSA ‡∏°‡∏µ‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡πÄ‡∏û‡∏¥‡πà‡∏° reward ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á ‡πÅ‡∏ï‡πà Q‚Äëvalue ‡πÉ‡∏ô‡∏ö‡∏£‡∏¥‡πÄ‡∏ß‡∏ì‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï termination ‡∏¢‡∏±‡∏á‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤\n",
        "**Deployment Performance (SARSA):**\n",
        "- Agent ‡∏ó‡∏µ‡πà‡∏ù‡∏∂‡∏Å‡∏î‡πâ‡∏ß‡∏¢ SARSA ‡∏û‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á‡∏Ç‡∏≠‡∏á cart ‡πÄ‡∏°‡∏∑‡πà‡∏≠ pole ‡∏™‡∏ß‡∏¥‡∏á‡∏≠‡∏≠‡∏Å‡∏ô‡∏≠‡∏Å‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï"
      ],
      "metadata": {
        "id": "1u1go1elExBO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Monte Carlo (MC)**\n",
        "‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÑ‡∏ü‡∏•‡πå reward ‡πÅ‡∏•‡∏∞ Q‚Äëvalue:"
      ],
      "metadata": {
        "id": "tnnI_Q1lEyqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MC_reward_file = \"reward_value/MC_r_0.json\"\n",
        "MC_q_value_file_1000 = \"q_value/Stabilize/MC/MC0/MC_0_1000_5_12_4_8.json\"\n",
        "MC_q_value_file_3000 = \"q_value/Stabilize/MC/MC0/MC_0_3000_5_12_4_8.json\"\n",
        "MC_q_value_file_5000 = \"q_value/Stabilize/MC/MC0/MC_0_5000_5_12_4_8.json\"\n",
        "\n",
        "plot_reward_grouped(MC_reward_file, 100)\n",
        "plot_q_values_3d_dual(load_q_values(MC_q_value_file_1000), (0,1), (4,8), (2,3), (4,4))\n",
        "plot_q_values_3d_dual(load_q_values(MC_q_value_file_3000), (0,1), (4,8), (2,3), (4,4))\n",
        "plot_q_values_3d_dual(load_q_values(MC_q_value_file_5000), (0,1), (4,8), (2,3), (4,4))"
      ],
      "metadata": {
        "id": "TFJUGUHqEzrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Learning Efficiency (MC):**\n",
        "- MC ‡∏°‡∏µ variance ‡∏™‡∏π‡∏á ‡∏ó‡∏≥‡πÉ‡∏´‡πâ Q‚Äëvalue ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏õ‡∏£‡∏õ‡∏£‡∏ß‡∏ô‡∏°‡∏≤‡∏Å ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠ discount factor ‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏™‡∏π‡∏á\n",
        "**Deployment Performance (MC):**\n",
        "- Agent ‡∏ó‡∏µ‡πà‡∏ù‡∏∂‡∏Å‡∏î‡πâ‡∏ß‡∏¢ MC ‡∏¢‡∏±‡∏á‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏° cart ‡πÉ‡∏´‡πâ‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤‡∏ó‡∏µ‡πà‡∏®‡∏π‡∏ô‡∏¢‡πå‡∏Å‡∏•‡∏≤‡∏á"
      ],
      "metadata": {
        "id": "wWOGVIF_E0vh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Additional Experiments**"
      ],
      "metadata": {
        "id": "tKoobzfXE14X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Agent Saturation Check**\n",
        "‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ reward over episode ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà converge ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏ß‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å 5000 episode ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ"
      ],
      "metadata": {
        "id": "meuUzOnOE2sB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Discount Factor Effect**\n",
        "‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏Ñ‡πà‡∏≤ Discount Factor ‡∏à‡∏≤‡∏Å 1 ‡πÑ‡∏õ‡πÄ‡∏õ‡πá‡∏ô 0.25 ‡πÅ‡∏•‡∏∞ 0.1 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏ú‡∏•‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÉ‡∏ô‡∏ß‡∏¥‡∏ò‡∏µ MC"
      ],
      "metadata": {
        "id": "2soN3YJ7E3iU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MC0_reward_file = \"reward_value/MC_r_0.json\"\n",
        "MC1_reward_file = \"reward_value/MC_r_1.json\"\n",
        "MC2_reward_file = \"reward_value/MC_r_2.json\"\n",
        "\n",
        "plot_multiple_reward_files([MC0_reward_file, MC1_reward_file, MC2_reward_file], 100)\n",
        "\n",
        "MC1_q_value_file_1000 = \"q_value/Stabilize/MC/MC1/MC_1_1000_5_12_4_8.json\"\n",
        "MC1_q_value_file_3000 = \"q_value/Stabilize/MC/MC1/MC_1_3000_5_12_4_8.json\"\n",
        "MC1_q_value_file_5000 = \"q_value/Stabilize/MC/MC1/MC_1_5000_5_12_4_8.json\"\n",
        "\n",
        "plot_q_values_3d_dual(load_q_values(MC1_q_value_file_1000), (0,1), (4,8), (2,3), (4,4))\n",
        "plot_q_values_3d_dual(load_q_values(MC1_q_value_file_3000), (0,1), (4,8), (2,3), (4,4))\n",
        "plot_q_values_3d_dual(load_q_values(MC1_q_value_file_5000), (0,1), (4,8), (2,3), (4,4))"
      ],
      "metadata": {
        "id": "7k46GTJ_E4Z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q-Learning with Larger Space**\n",
        "‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏° resolution ‡∏Ç‡∏≠‡∏á state ‡πÅ‡∏•‡∏∞ action space ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏ú‡∏•‡∏ï‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á agent"
      ],
      "metadata": {
        "id": "wDSIYtd0E5YY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ql_mod_q_file_1000 = \"q_value/Stabilize/Q_Learning/Q_Learning1/Q_Learning_1_1000_11_25_7_12.json\"\n",
        "ql_mod_q_file_3000 = \"q_value/Stabilize/Q_Learning/Q_Learning1/Q_Learning_1_3000_11_25_7_12.json\"\n",
        "ql_mod_q_file_5000 = \"q_value/Stabilize/Q_Learning/Q_Learning1/Q_Learning_1_5000_11_25_7_12.json\"\n",
        "ql_mod_q_file_10000 = \"q_value/Stabilize/Q_Learning/Q_Learning1/Q_Learning_1_10000_11_25_7_12.json\"\n",
        "ql_mod_q_file_15000 = \"q_value/Stabilize/Q_Learning/Q_Learning1/Q_Learning_1_15000_11_25_7_12.json\"\n",
        "ql_mod_q_file_20000 = \"q_value/Stabilize/Q_Learning/Q_Learning1/Q_Learning_1_20000_11_25_7_12.json\"\n",
        "ql_mod_q_file = [ql_mod_q_file_1000, ql_mod_q_file_3000, ql_mod_q_file_5000, ql_mod_q_file_10000, ql_mod_q_file_15000, ql_mod_q_file_20000]\n",
        "ql_mod_episode_labels = [f\"Episode {ep}\" for ep in [1000, 3000, 5000, 10000, 15000, 20000]]\n",
        "\n",
        "plot_q_values_heatmaps_multiple_files(ql_mod_q_file, episodes_list=ql_mod_episode_labels, left_pair=(0,1), left_range=(7,12), right_pair=(2,3), right_range=(5,5))\n",
        "\n",
        "plot_q_values_3d_dual(load_q_values(ql_mod_q_file_20000), (0,1), (7,12), (2,3), (5,5))"
      ],
      "metadata": {
        "id": "L8LuZjIgE6J4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Comparisons**\n",
        "### ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ú‡∏• reward ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Double Q-Learning ‡πÅ‡∏•‡∏∞ Q-Learning"
      ],
      "metadata": {
        "id": "ngFp0DMmE7jG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_multiple_reward_files([\"reward_value/QL_r_1.json\", \"reward_value/Double_Q_learning_r_1.json\"], 100)"
      ],
      "metadata": {
        "id": "cyF2B3oqE8rK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Learning Rate Comparison**\n",
        "‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Ñ‡πà‡∏≤ learning rate 0.03 ‡∏Å‡∏±‡∏ö 0.1 ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ"
      ],
      "metadata": {
        "id": "0JXDsB_HFSaV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Learning Objectives:**\n",
        "- ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Ç‡∏≠‡∏á agent ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡πÉ‡∏ô‡∏™‡∏†‡∏≤‡∏ß‡∏∞‡∏ó‡∏µ‡πà dynamic model ‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö\n",
        "- ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏î‡∏µ‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ RL algorithm (Monte Carlo, SARSA, Q-Learning, Double Q-Learning)\n",
        "- ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏Ç‡∏≠‡∏á resolution ‡∏Ç‡∏≠‡∏á state ‡πÅ‡∏•‡∏∞ action space ‡∏ï‡πà‡∏≠‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ"
      ],
      "metadata": {
        "id": "UMiIZhg8FUKK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compare Lower Space and Higher Space:**\n",
        "- State/action space ‡∏ó‡∏µ‡πà‡∏ï‡πà‡∏≥‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ agent ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÑ‡∏î‡πâ‡∏á‡πà‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏°‡∏µ state-action pair ‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤ ‡πÅ‡∏ï‡πà‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏≠‡∏≤‡∏à‡πÑ‡∏°‡πà‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î\n",
        "- ‡πÉ‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏•‡∏±‡∏ö‡∏Å‡∏±‡∏ô state/action space ‡∏ó‡∏µ‡πà‡∏™‡∏π‡∏á‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ agent ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à environment ‡πÑ‡∏î‡πâ‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Ç‡∏∂‡πâ‡∏ô ‡πÅ‡∏ï‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ episode ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô\n",
        "- ‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å resolution ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏à‡∏∂‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£ trade-off ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°"
      ],
      "metadata": {
        "id": "w2VkO9GWFVBJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question:**\n",
        "### 1. **Which algorithm performs best?**\n",
        "   - ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡πÅ‡∏™‡∏î‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤ ...\n",
        "### 2. **Why does it perform better than the others?**\n",
        "   - ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å ...\n",
        "### 3. **How do the resolutions of the action space and observation space affect the learning process? Why?**\n",
        "   - ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ resolution ‡∏ó‡∏µ‡πà‡∏™‡∏π‡∏á‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ agent ‡πÑ‡∏î‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Ç‡∏∂‡πâ‡∏ô ‡πÅ‡∏ï‡πà‡∏Å‡πá‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï state-action pair ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô ‡∏ã‡∏∂‡πà‡∏á‡∏™‡πà‡∏á‡∏ú‡∏•‡πÉ‡∏´‡πâ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ episode ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô"
      ],
      "metadata": {
        "id": "yc6wFzRXFWEo"
      }
    }
  ]
}